{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fairseq_for_ch_en.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMe+vvEdD3Zuc/0D8Vq0yh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxs131/NMT-example/blob/main/fairseq_for_ch_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#translation\n",
        "本文主要使用fairseq库进行翻译\n",
        "*     数据集使用WMT中英模型  news-commentary-v15 下载地址为 https://data.statmt.org/news-commentary/v15/training/\n",
        "\n",
        "*     工具包：Moses(SMT工具，tokensation ,truecasing,cleaning）\n",
        "*         subword-nmt 使用BPE算法生成子词\n",
        "*         jieba 中文分词"
      ],
      "metadata": {
        "id": "UyJ7FZjUcPas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#安装工具包和下载数据"
      ],
      "metadata": {
        "id": "BaaKYqPddO5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/moses-smt/mosesdecoder.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwd1yvwAdR0U",
        "outputId": "f7b21abc-ed2f-40c5-989d-b00d66831abd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148090, done.\u001b[K\n",
            "remote: Counting objects: 100% (518/518), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 148090 (delta 319), reused 443 (delta 292), pack-reused 147572\u001b[K\n",
            "Receiving objects: 100% (148090/148090), 129.87 MiB | 19.90 MiB/s, done.\n",
            "Resolving deltas: 100% (114345/114345), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rsennrich/subword-nmt.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1NOBkmedeQL",
        "outputId": "3b103e85-bd96-404e-cfbb-723924858c15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 587, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 587 (delta 1), reused 4 (delta 1), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (587/587), 244.20 KiB | 3.34 MiB/s, done.\n",
            "Resolving deltas: 100% (350/350), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swr1x9sBd2Cb",
        "outputId": "1b054b96-9ca5-4eb0-f359-dfeaca452bd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 30909, done.\u001b[K\n",
            "remote: Counting objects: 100% (541/541), done.\u001b[K\n",
            "remote: Compressing objects: 100% (329/329), done.\u001b[K\n",
            "remote: Total 30909 (delta 233), reused 409 (delta 202), pack-reused 30368\u001b[K\n",
            "Receiving objects: 100% (30909/30909), 21.20 MiB | 23.44 MiB/s, done.\n",
            "Resolving deltas: 100% (22928/22928), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --editable fairseq/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_Y0dUIleBXC",
        "outputId": "e2e92758-e8f2-4d5d-f998-ed1d0d19725e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (1.10.0+cu111)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (1.21.5)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting bitarray\n",
            "  Downloading bitarray-2.3.7.tar.gz (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (4.62.3)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (1.15.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (0.10.0+cu111)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (0.29.27)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+5b87224) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+5b87224) (3.10.0.2)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.6 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+5b87224) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+5b87224) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+5b87224) (3.7.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, bitarray\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=0d978be28f83f43b3f2c30b061ce12f5ecbf701e5a1d184b507524b8ae0a48f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for bitarray (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bitarray: filename=bitarray-2.3.7-cp37-cp37m-linux_x86_64.whl size=173583 sha256=13999d56d541129fc9b704f137e670dc366b56458694a333358ae183b40ddd67\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/65/b0/59479ecb406b1769a3126c2a633aad7365b956373d79724ef4\n",
            "Successfully built antlr4-python3-runtime bitarray\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.3.7 colorama-0.4.4 fairseq hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.4.0 sacrebleu-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#安装apex\n",
        "APEX是英伟达开源，支持pytorch框架，通过改变数据格式来减少模型占用显存的工具。\n",
        "最有价值的是amp（automatic mixed precision）将大部分操作使用float16，部分保留float32，可以增大batch_size，带来精度上的提升和训练速度的提升。\n",
        "\n",
        "一般的fairseq训练翻译器都会使用apex"
      ],
      "metadata": {
        "id": "nPBzN8aGV5tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install -v --no-cache-dir --global-option='--cpp_ext' --global-option='--cuda_ext' apex/."
      ],
      "metadata": {
        "id": "Mo4dD2j4Wiye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#验证是否安装成果\n",
        "from apex import amp"
      ],
      "metadata": {
        "id": "dIDS875IXy0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#将fairseq加入python环境中，否则后面fariseq-proprecess和fairseq-train会报错，不是colab环境可以忽略此次\n",
        "!echo $PYTHONPATH\n",
        "\n",
        "import os \n",
        "os.environ['PYTHONPATH']+=':/content/fairseq'\n",
        "#os.environ['PYTHONPATH']+=':/content/apex'\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v3O4zh7OJiI",
        "outputId": "ee8a45a9-afc8-4549-f717-ef47f91b4103"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python\n",
            "/env/python:/content/fairseq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#构建目录结构\n",
        "!mkdir nmt\n",
        "!mkdir nmt/data\n",
        "!mkdir nmt/model\n",
        "!mkdir nmt/utils\n",
        "!mkdir nmt/scripts\n",
        "!mkdir nmt/model/checkpoints"
      ],
      "metadata": {
        "id": "DmCTlahOeXK0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#下载数据集\n",
        "!wget https://data.statmt.org/news-commentary/v15/training/news-commentary-v15.en-zh.tsv.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PlfpgZRe1NU",
        "outputId": "ed699a9e-1fa2-44f0-8fb2-67d773ebeaf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-22 10:32:04--  https://data.statmt.org/news-commentary/v15/training/news-commentary-v15.en-zh.tsv.gz\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36737639 (35M) [application/x-gzip]\n",
            "Saving to: ‘news-commentary-v15.en-zh.tsv.gz’\n",
            "\n",
            "news-commentary-v15 100%[===================>]  35.04M  17.3MB/s    in 2.0s    \n",
            "\n",
            "2022-02-22 10:32:07 (17.3 MB/s) - ‘news-commentary-v15.en-zh.tsv.gz’ saved [36737639/36737639]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#解压数据集\n",
        "!gunzip news-commentary-v15.en-zh.tsv.gz"
      ],
      "metadata": {
        "id": "ABxZwK_-feir"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv news-commentary-v15.en-zh.tsv nmt/data/news-commentary-v15.en-zh.tsv"
      ],
      "metadata": {
        "id": "TNnYLC5of8pU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#bash文件\n",
        "编写sh文件，定义了后面需要用到的变量，脚本路径,这里由于colab原因，没有使用，大家可以简单浏览以下，有一部分我也修改了。\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6Hp5EsFgNtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "def cut(fpath,new_data_dir,nsrc='zh',ntgt='en'):\n",
        "    fp=open(fpath,encoding='utf-8')\n",
        "    src_fp=open(os.path.join(new_data_dir,'raw.'+nsrc),'w',encoding='utf-8')\n",
        "    tgt_fp=open(os.path.join(new_data_dir,'raw.'+ntgt),'w',encoding='utf-8')\n",
        "\n",
        "    for line in fp.readlines():\n",
        "        tgt_line,src_line=line.replace('\\n','').split('\\t')\n",
        "        src_fp.write(src_line+'\\n')\n",
        "        tgt_fp.write(tgt_line+'\\n')\n",
        "\n",
        "    src_fp.close()\n",
        "    tgt_fp.close()\n",
        "\n",
        "\n",
        "cut('nmt/data/news-commentary-v15.en-zh.tsv','nmt/data/')"
      ],
      "metadata": {
        "id": "sIxNAOMJjKOr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 nmt/data/raw.en\n",
        "!head -n 5 nmt/data/raw.zh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yrmtY37leVb",
        "outputId": "b4d2a188-afec-4a1b-cdcb-9f346e0e55ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1929 or 1989?\n",
            "PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n",
            "At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.\n",
            "Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.\n",
            "The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).\n",
            "1929年还是1989年?\n",
            "巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
            "一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。\n",
            "如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。\n",
            "目前的趋势是，要么是过度的克制（欧洲 ） ， 要么是努力的扩展（美国 ） 。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据预处理\n",
        "分以下几步进行\n",
        "\n",
        "*     标点符号标准化\n",
        "*     jieba中文分词\n",
        "*     tokenize 英文单词与标点分开，转换连续空格，符号变为转义字符\n",
        "*     truecase 转换大小写为适合学习的形式\n",
        "*     将分词后结果细分为BPE子词\n",
        "*     clean 对语料截取长度，最大长度到最小长度之间（1-256）\n",
        "*     split 划分训练，验证，测试集\n"
      ],
      "metadata": {
        "id": "1qiu7yQLmHFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#标点符号的标准化\n",
        "\n",
        "!perl mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en < nmt/data/raw.en > nmt/data/norm.en\n",
        "!perl mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l zh < nmt/data/raw.zh > nmt/data/norm.zh"
      ],
      "metadata": {
        "id": "OUiPyWFD_lyz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#中文分词\n",
        "!python -m jieba -d ' ' nmt/data/norm.zh > nmt/data/norm.seg.zh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlxfEbXRBgSC",
        "outputId": "da52b74f-4d56-4aba-8e9c-2636585dfebd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.915 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 nmt/data/norm.seg.zh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex1puY19Bv1Z",
        "outputId": "5e706229-8bc1-4107-d1a2-6f9aee4919f5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1929 年 还是 1989 年 ?\n",
            "巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。\n",
            "一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽心 的 ， 因为 这 两段 时期 意味着 典型 的 周期性 衰退 。\n",
            "如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 年 和 1931 年 相比 ， 即使 一些 国家 政府 的 表现 仍然 似乎 把视 目前 的 情况 为 是 典型 的 而 看见 的 衰退 。\n",
            "目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲   ）   ，   要么 是 努力 的 扩展 （ 美国   ）   。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize 分词，将符号替换为转义字符\n",
        "!mosesdecoder/scripts/tokenizer/tokenizer.perl -l en <nmt/data/norm.en >nmt/data/norm.tok.en\n",
        "!mosesdecoder/scripts/tokenizer/tokenizer.perl -l zh <nmt/data/norm.seg.zh >nmt/data/norm.tok.zh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUBtJMrcB9Ta",
        "outputId": "29ded00d-6265-4a51-9ac3-889a4d7c8a36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: zh\n",
            "Number of threads: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#转换大小写\n",
        "!mosesdecoder/scripts/recaser/train-truecaser.perl --model nmt/model/truecase-model.en --corpus nmt/data/norm.tok.en\n",
        "!mosesdecoder/scripts/recaser/truecase.perl --model nmt/model/truecase-model.en < nmt/data/norm.tok.en > nmt/data/norm.tok.true.en"
      ],
      "metadata": {
        "id": "GPyZKmBzC-Vx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "id": "Qze0Pbj-GRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE划分子词\n",
        "!python subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input nmt/data/norm.tok.true.en -s 32000 -o nmt/data/bpecode.en --write-vocabulary nmt/data/voc.en\n",
        "!python subword-nmt/subword_nmt/apply_bpe.py -c nmt/data/bpecode.en --vocabulary nmt/data/voc.en <nmt/data/norm.tok.true.en >nmt/data/bpe.en\n",
        "\n",
        "!python subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input nmt/data/norm.seg.zh -s 32000 -o nmt/data/bpecode.zh --write-vocabulary nmt/data/voc.zh\n",
        "!python subword-nmt/subword_nmt/apply_bpe.py -c nmt/data/bpecode.zh --vocabulary nmt/data/voc.zh <nmt/data/norm.seg.zh >nmt/data/bpe.zh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX5JMACmEL2c",
        "outputId": "c437cc1b-44c2-4a50-c6e8-0d4870c6e552"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 32000/32000 [00:52<00:00, 609.76it/s]\n",
            "100% 32000/32000 [00:52<00:00, 605.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 nmt/data/bpe.en\n",
        "!head -n 5 nmt/data/bpe.zh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EQSo3X7FXNa",
        "outputId": "b8111a6d-2327-47a7-d9f8-ce56d7443f28"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1929 or 1989 ?\n",
            "Paris - As the economic crisis deepens and wid@@ ens , the world has been searching for historical analogies to help us understand what has been happening .\n",
            "at the start of the crisis , many people likened it to 1982 or 1973 , which was reassuring , because both dates refer to classical cyclical downturns .\n",
            "today , the mood is much gri@@ mmer , with references to 1929 and 1931 beginning to abound , even if some governments continue to behave as if the crisis was more classical than exceptional .\n",
            "the tendency is either excessive restraint ( Europe ) or a diffusion of the effort ( the United States ) .\n",
            "1929 年 还是 1989 年 ?\n",
            "巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的 类似 事件 希望 有助于 我们 了解 目前 正在 发生 的 情况 。\n",
            "一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 情况 ， 这样 得 类比 是 令人 宽@@ 心 的 ， 因为 这 两@@ 段 时期 意味着 典型 的 周期性 衰退 。\n",
            "如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 年 和 1931 年 相比 ， 即使 一些 国家 政府 的 表现 仍然 似乎 把@@ 视 目前 的 情况 为 是 典型 的 而 看见 的 衰退 。\n",
            "目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ） ， 要么 是 努力 的 扩展 （ 美国 ） 。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clean 过滤空白行和超过256长度的句子\n",
        "!mosesdecoder/scripts/training/clean-corpus-n.perl nmt/data/bpe zh en nmt/data/clean 1 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-oYRC85F6aB",
        "outputId": "6c9f1477-1490-483c-a6df-4c10fc90e02d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clean-corpus.perl: processing nmt/data/bpe.zh & .en to nmt/data/clean, cutoff 1-256, ratio 9\n",
            "..........(100000)..........(200000)..........(300000)..\n",
            "Input sentences: 320713  Output sentences:  312215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split 划分为训练\n",
        "import random \n",
        "\n",
        "def split(src_fpath,tgt_fpath,nsrc='zh',ntgt='en',ratio=(0.9,0.05,0.05),new_data_dir='nmt/data'):\n",
        "  src_fp=open(src_fpath,'r',encoding='utf-8')\n",
        "  tgt_fp=open(tgt_fpath,'r',encoding='utf-8')\n",
        "\n",
        "  src_train,src_valid,src_test=open(os.path.join(new_data_dir,'train.'+nsrc),'w',encoding='utf-8'),open(os.path.join(new_data_dir,'valid.'+nsrc),'w',encoding='utf-8'),open(os.path.join(new_data_dir,'test.'+nsrc),'w',encoding='utf-8')\n",
        "  tgt_train,tgt_valid,tgt_test=open(os.path.join(new_data_dir,'train.'+ntgt),'w',encoding='utf-8'),open(os.path.join(new_data_dir,'valid.'+ntgt),'w',encoding='utf-8'),open(os.path.join(new_data_dir,'test.'+ntgt),'w',encoding='utf-8')\n",
        "\n",
        "  src,tgt=src_fp.readlines(),tgt_fp.readlines()\n",
        "\n",
        "  for s,t in zip(src,tgt):\n",
        "    rand=random.random()\n",
        "    if 0<rand<=ratio[0]:\n",
        "      src_train.write(s)\n",
        "      tgt_train.write(t)\n",
        "    elif ratio[0]<rand<=ratio[0]+ratio[1]:\n",
        "      src_valid.write(s)\n",
        "      tgt_valid.write(t)\n",
        "    else:\n",
        "      src_test.write(s)\n",
        "      tgt_test.write(t)\n",
        "\n",
        "  src_fp.close()\n",
        "  tgt_fp.close()\n",
        "  src_train.close()\n",
        "  tgt_train.close()\n",
        "  src_valid.close()\n",
        "  tgt_valid.close()\n",
        "  src_test.close()\n",
        "  tgt_test.close()\n",
        "\n",
        "split('nmt/data/clean.zh','nmt/data/clean.en',)\n"
      ],
      "metadata": {
        "id": "6J_5LtPKGzOp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fairseq训练过程\n",
        "首先使用fairseq-preprocess命令生成词表和训练用二进制文件"
      ],
      "metadata": {
        "id": "v6fZXXZIKqRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess --source-lang zh --target-lang en \\\n",
        "    --trainpref nmt/data/train --validpref nmt/data/valid --testpref nmt/data/test --destdir nmt/data/data-bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8h7kwTbK8_H",
        "outputId": "2dc22a89-8785-486a-98a2-d9d62143c892"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-22 11:38:39 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-02-22 11:38:40 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='nmt/data/data-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='zh', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='nmt/data/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='nmt/data/train', use_plasma_view=False, user_dir=None, validpref='nmt/data/valid', wandb_project=None, workers=1)\n",
            "2022-02-22 11:39:40 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 37080 types\n",
            "2022-02-22 11:40:54 | INFO | fairseq_cli.preprocess | [zh] nmt/data/train.zh: 280793 sents, 7125108 tokens, 0.0% replaced (by <unk>)\n",
            "2022-02-22 11:40:54 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 37080 types\n",
            "2022-02-22 11:40:58 | INFO | fairseq_cli.preprocess | [zh] nmt/data/valid.zh: 15792 sents, 399464 tokens, 0.0233% replaced (by <unk>)\n",
            "2022-02-22 11:40:58 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 37080 types\n",
            "2022-02-22 11:41:02 | INFO | fairseq_cli.preprocess | [zh] nmt/data/test.zh: 15630 sents, 397884 tokens, 0.0196% replaced (by <unk>)\n",
            "2022-02-22 11:41:02 | INFO | fairseq_cli.preprocess | [en] Dictionary: 30864 types\n",
            "2022-02-22 11:41:53 | INFO | fairseq_cli.preprocess | [en] nmt/data/train.en: 280793 sents, 7560436 tokens, 0.0% replaced (by <unk>)\n",
            "2022-02-22 11:41:53 | INFO | fairseq_cli.preprocess | [en] Dictionary: 30864 types\n",
            "2022-02-22 11:41:56 | INFO | fairseq_cli.preprocess | [en] nmt/data/valid.en: 15792 sents, 422894 tokens, 0.00828% replaced (by <unk>)\n",
            "2022-02-22 11:41:56 | INFO | fairseq_cli.preprocess | [en] Dictionary: 30864 types\n",
            "2022-02-22 11:41:59 | INFO | fairseq_cli.preprocess | [en] nmt/data/test.en: 15630 sents, 422984 tokens, 0.00993% replaced (by <unk>)\n",
            "2022-02-22 11:41:59 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to nmt/data/data-bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#训练,为了提前结束，这里min-loss-scale 当loss降低到0.5就结束训练，默认值为0.0001，max-tokens 一般设为4000，这里设置为2048\n",
        "!fairseq-train nmt/data/data-bin  --arch transformer \\\n",
        "                  --source-lang zh --target-lang en\\\n",
        "                  --optimizer adam --lr 0.001 --adam-betas '(0.9,0.98)' \\\n",
        "                  --lr-scheduler inverse_sqrt --max-tokens 2048 --dropout 0.3 \\\n",
        "                  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "                  --min-loss-scale 0.5\\\n",
        "                  --keep-last-epochs 2 --num-workers 8\\\n",
        "                  --save-dir nmt/model/checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBq7tIgdLbiK",
        "outputId": "3094d18d-e544-433b-f6c2-99d7b68fada2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-22 11:51:08 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-02-22 11:51:09 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-02-22 11:51:12 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.5, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'nmt/model/checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='nmt/data/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.001], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.5, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=8, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='nmt/model/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='zh', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'nmt/data/data-bin', 'source_lang': 'zh', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-22 11:51:12 | INFO | fairseq.tasks.translation | [zh] dictionary: 37080 types\n",
            "2022-02-22 11:51:12 | INFO | fairseq.tasks.translation | [en] dictionary: 30864 types\n",
            "2022-02-22 11:51:13 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(37080, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(30864, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=30864, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-02-22 11:51:13 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-02-22 11:51:13 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-02-22 11:51:13 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-02-22 11:51:13 | INFO | fairseq_cli.train | num. shared model params: 94,728,192 (num. trained: 94,728,192)\n",
            "2022-02-22 11:51:13 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-02-22 11:51:13 | INFO | fairseq.data.data_utils | loaded 15,792 examples from: nmt/data/data-bin/valid.zh-en.zh\n",
            "2022-02-22 11:51:13 | INFO | fairseq.data.data_utils | loaded 15,792 examples from: nmt/data/data-bin/valid.zh-en.en\n",
            "2022-02-22 11:51:13 | INFO | fairseq.tasks.translation | nmt/data/data-bin valid zh-en 15792 examples\n",
            "2022-02-22 11:51:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-02-22 11:51:23 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               \n",
            "2022-02-22 11:51:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-02-22 11:51:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-02-22 11:51:23 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None\n",
            "2022-02-22 11:51:23 | INFO | fairseq.trainer | Preparing to load checkpoint nmt/model/checkpoints/checkpoint_last.pt\n",
            "2022-02-22 11:51:23 | INFO | fairseq.trainer | No existing checkpoint found nmt/model/checkpoints/checkpoint_last.pt\n",
            "2022-02-22 11:51:23 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-02-22 11:51:23 | INFO | fairseq.data.data_utils | loaded 280,793 examples from: nmt/data/data-bin/train.zh-en.zh\n",
            "2022-02-22 11:51:23 | INFO | fairseq.data.data_utils | loaded 280,793 examples from: nmt/data/data-bin/train.zh-en.en\n",
            "2022-02-22 11:51:23 | INFO | fairseq.tasks.translation | nmt/data/data-bin train zh-en 280793 examples\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2022-02-22 11:51:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4184\n",
            "epoch 001:   0% 0/4184 [00:00<?, ?it/s]2022-02-22 11:51:23 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-02-22 11:51:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:375: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "epoch 001: 100% 4183/4184 [35:20<00:00,  1.91it/s, loss=8.328, nll_loss=7.302, ppl=157.79, wps=3592, ups=2.01, wpb=1783.3, bsz=62.3, num_updates=4100, lr=0.00098773, gnorm=1.071, train_wall=49, gb_free=8.5, wall=2080]2022-02-22 12:26:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/265 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   0% 1/265 [00:00<02:34,  1.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 2/265 [00:00<01:29,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   1% 3/265 [00:00<01:05,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 4/265 [00:01<00:53,  4.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 5/265 [00:01<00:49,  5.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 6/265 [00:01<00:46,  5.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 7/265 [00:01<00:45,  5.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 9/265 [00:01<00:36,  7.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 10/265 [00:01<00:37,  6.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 11/265 [00:02<00:36,  6.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 12/265 [00:02<00:36,  6.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 13/265 [00:02<00:38,  6.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 14/265 [00:02<00:39,  6.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 15/265 [00:02<00:38,  6.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 16/265 [00:02<00:39,  6.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 17/265 [00:02<00:39,  6.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 18/265 [00:03<00:38,  6.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 19/265 [00:03<00:40,  6.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 20/265 [00:03<00:41,  5.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 21/265 [00:03<00:40,  6.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 22/265 [00:03<00:39,  6.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 23/265 [00:03<00:40,  6.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 24/265 [00:04<00:40,  5.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 25/265 [00:04<00:40,  5.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 26/265 [00:04<00:38,  6.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 27/265 [00:04<00:37,  6.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 28/265 [00:04<00:39,  6.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 29/265 [00:04<00:39,  6.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 30/265 [00:05<00:38,  6.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 31/265 [00:05<00:37,  6.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 32/265 [00:05<00:37,  6.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 33/265 [00:05<00:39,  5.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 34/265 [00:05<00:39,  5.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 35/265 [00:05<00:38,  6.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 36/265 [00:06<00:37,  6.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 37/265 [00:06<00:35,  6.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 38/265 [00:06<00:36,  6.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 39/265 [00:06<00:38,  5.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 40/265 [00:06<00:37,  6.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 41/265 [00:06<00:37,  6.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 42/265 [00:07<00:36,  6.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 43/265 [00:07<00:34,  6.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 44/265 [00:07<00:33,  6.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 45/265 [00:07<00:36,  6.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 46/265 [00:07<00:37,  5.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 47/265 [00:07<00:37,  5.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 48/265 [00:08<00:36,  5.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 49/265 [00:08<00:35,  6.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  19% 50/265 [00:08<00:34,  6.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  19% 51/265 [00:08<00:36,  5.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 52/265 [00:08<00:36,  5.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 53/265 [00:08<00:36,  5.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 54/265 [00:09<00:35,  5.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 55/265 [00:09<00:35,  5.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 56/265 [00:09<00:34,  6.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 57/265 [00:09<00:30,  6.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 58/265 [00:09<00:32,  6.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 59/265 [00:09<00:33,  6.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 60/265 [00:10<00:33,  6.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 61/265 [00:10<00:33,  6.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 62/265 [00:10<00:32,  6.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 63/265 [00:10<00:31,  6.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 65/265 [00:10<00:27,  7.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 66/265 [00:10<00:29,  6.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 67/265 [00:11<00:30,  6.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 68/265 [00:11<00:31,  6.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 69/265 [00:11<00:31,  6.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 70/265 [00:11<00:31,  6.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 71/265 [00:11<00:31,  6.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 72/265 [00:11<00:31,  6.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 73/265 [00:12<00:31,  6.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 74/265 [00:12<00:32,  5.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  28% 75/265 [00:12<00:32,  5.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 76/265 [00:12<00:31,  5.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 77/265 [00:12<00:31,  5.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 78/265 [00:12<00:30,  6.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 79/265 [00:13<00:28,  6.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 80/265 [00:13<00:29,  6.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 81/265 [00:13<00:30,  6.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 82/265 [00:13<00:31,  5.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 83/265 [00:13<00:31,  5.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 84/265 [00:13<00:30,  5.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 85/265 [00:14<00:29,  6.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 86/265 [00:14<00:28,  6.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 87/265 [00:14<00:27,  6.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 88/265 [00:14<00:28,  6.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 89/265 [00:14<00:29,  6.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 90/265 [00:14<00:28,  6.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  34% 91/265 [00:15<00:28,  6.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 92/265 [00:15<00:27,  6.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 93/265 [00:15<00:27,  6.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 94/265 [00:15<00:26,  6.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 95/265 [00:15<00:25,  6.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 96/265 [00:15<00:26,  6.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 97/265 [00:16<00:27,  6.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 98/265 [00:16<00:28,  5.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 99/265 [00:16<00:27,  6.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 100/265 [00:16<00:27,  6.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 101/265 [00:16<00:26,  6.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 102/265 [00:16<00:25,  6.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 103/265 [00:16<00:25,  6.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 104/265 [00:17<00:25,  6.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 105/265 [00:17<00:26,  6.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 106/265 [00:17<00:26,  5.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 107/265 [00:17<00:25,  6.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 108/265 [00:17<00:25,  6.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 109/265 [00:17<00:25,  6.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 110/265 [00:18<00:25,  6.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 111/265 [00:18<00:24,  6.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 112/265 [00:18<00:24,  6.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 113/265 [00:18<00:24,  6.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 114/265 [00:18<00:25,  5.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 115/265 [00:18<00:24,  6.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 116/265 [00:19<00:24,  6.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 117/265 [00:19<00:24,  5.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 118/265 [00:19<00:24,  6.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 119/265 [00:19<00:23,  6.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 120/265 [00:19<00:21,  6.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 121/265 [00:19<00:23,  6.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 122/265 [00:20<00:23,  5.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 123/265 [00:20<00:24,  5.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 124/265 [00:20<00:24,  5.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 125/265 [00:20<00:23,  5.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 126/265 [00:20<00:22,  6.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 127/265 [00:20<00:21,  6.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 128/265 [00:21<00:20,  6.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 129/265 [00:21<00:20,  6.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 130/265 [00:21<00:21,  6.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 131/265 [00:21<00:22,  6.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 132/265 [00:21<00:21,  6.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 133/265 [00:21<00:21,  6.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 134/265 [00:22<00:21,  6.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 135/265 [00:22<00:20,  6.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 136/265 [00:22<00:20,  6.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 137/265 [00:22<00:21,  5.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 138/265 [00:22<00:21,  5.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 139/265 [00:22<00:21,  5.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 140/265 [00:23<00:21,  5.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 141/265 [00:23<00:20,  6.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 142/265 [00:23<00:19,  6.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 143/265 [00:23<00:19,  6.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 144/265 [00:23<00:19,  6.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 145/265 [00:23<00:20,  5.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 146/265 [00:24<00:19,  5.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 147/265 [00:24<00:19,  5.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 148/265 [00:24<00:19,  5.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 149/265 [00:24<00:19,  6.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 150/265 [00:24<00:18,  6.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 151/265 [00:24<00:17,  6.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 152/265 [00:25<00:18,  6.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 153/265 [00:25<00:18,  6.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 154/265 [00:25<00:18,  5.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 155/265 [00:25<00:17,  6.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 156/265 [00:25<00:17,  6.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 157/265 [00:25<00:17,  6.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 158/265 [00:25<00:16,  6.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 159/265 [00:26<00:17,  6.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 160/265 [00:26<00:17,  6.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 161/265 [00:26<00:17,  5.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 162/265 [00:26<00:17,  5.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 163/265 [00:26<00:16,  6.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 164/265 [00:26<00:16,  6.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 165/265 [00:27<00:16,  6.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 166/265 [00:27<00:16,  5.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 167/265 [00:27<00:17,  5.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 168/265 [00:27<00:16,  5.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 169/265 [00:27<00:16,  5.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 170/265 [00:27<00:15,  6.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 171/265 [00:28<00:15,  6.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 172/265 [00:28<00:15,  5.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 173/265 [00:28<00:15,  5.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 174/265 [00:28<00:15,  5.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 175/265 [00:28<00:15,  5.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 176/265 [00:28<00:14,  6.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 177/265 [00:29<00:13,  6.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 178/265 [00:29<00:14,  6.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 179/265 [00:29<00:14,  5.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 180/265 [00:29<00:14,  5.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 181/265 [00:29<00:14,  5.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 182/265 [00:29<00:13,  5.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 183/265 [00:30<00:13,  6.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 184/265 [00:30<00:12,  6.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 185/265 [00:30<00:12,  6.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 186/265 [00:30<00:13,  6.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 187/265 [00:30<00:12,  6.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 188/265 [00:30<00:11,  6.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 189/265 [00:31<00:10,  6.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 190/265 [00:31<00:11,  6.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 191/265 [00:31<00:11,  6.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 192/265 [00:31<00:12,  5.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 193/265 [00:31<00:11,  6.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 194/265 [00:31<00:11,  6.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 195/265 [00:32<00:11,  6.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 196/265 [00:32<00:11,  5.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 197/265 [00:32<00:10,  6.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 198/265 [00:32<00:10,  6.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 199/265 [00:32<00:10,  6.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 200/265 [00:32<00:10,  6.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 201/265 [00:33<00:10,  5.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 202/265 [00:33<00:10,  6.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 203/265 [00:33<00:10,  6.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 204/265 [00:33<00:09,  6.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 205/265 [00:33<00:09,  6.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 206/265 [00:33<00:09,  6.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 207/265 [00:33<00:09,  6.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 208/265 [00:34<00:09,  6.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 209/265 [00:34<00:08,  6.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 210/265 [00:34<00:08,  6.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 211/265 [00:34<00:08,  6.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 212/265 [00:34<00:08,  6.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 213/265 [00:34<00:07,  6.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 214/265 [00:35<00:07,  6.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  81% 215/265 [00:35<00:07,  6.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 216/265 [00:35<00:07,  6.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 217/265 [00:35<00:07,  6.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 218/265 [00:35<00:07,  6.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 219/265 [00:35<00:07,  6.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 220/265 [00:36<00:07,  6.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 221/265 [00:36<00:06,  6.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 222/265 [00:36<00:06,  6.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  84% 223/265 [00:36<00:06,  6.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 224/265 [00:36<00:06,  6.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 225/265 [00:36<00:06,  6.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 226/265 [00:36<00:06,  6.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 227/265 [00:37<00:05,  6.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 228/265 [00:37<00:05,  6.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 229/265 [00:37<00:05,  7.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 230/265 [00:37<00:05,  6.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 231/265 [00:37<00:05,  6.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 232/265 [00:37<00:04,  6.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 233/265 [00:37<00:04,  6.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 234/265 [00:38<00:04,  6.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 235/265 [00:38<00:04,  6.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 236/265 [00:38<00:04,  6.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 237/265 [00:38<00:04,  6.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 238/265 [00:38<00:03,  6.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 239/265 [00:38<00:03,  6.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 240/265 [00:38<00:03,  7.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 241/265 [00:39<00:03,  6.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 242/265 [00:39<00:03,  6.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 243/265 [00:39<00:03,  6.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 244/265 [00:39<00:03,  6.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 245/265 [00:39<00:03,  6.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 246/265 [00:39<00:02,  6.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 247/265 [00:40<00:02,  6.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 248/265 [00:40<00:02,  6.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 249/265 [00:40<00:02,  6.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 250/265 [00:40<00:02,  6.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 251/265 [00:40<00:02,  6.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 252/265 [00:40<00:01,  6.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 253/265 [00:40<00:01,  7.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 254/265 [00:41<00:01,  6.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 255/265 [00:41<00:01,  6.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 256/265 [00:41<00:01,  6.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 257/265 [00:41<00:01,  6.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 258/265 [00:41<00:01,  6.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 259/265 [00:41<00:00,  6.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 260/265 [00:42<00:00,  6.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 261/265 [00:42<00:00,  6.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 262/265 [00:42<00:00,  7.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  99% 263/265 [00:42<00:00,  7.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 264/265 [00:42<00:00,  7.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 265/265 [00:42<00:00,  7.79it/s]\u001b[A\n",
            "                                                                          \u001b[A2022-02-22 12:27:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.247 | nll_loss 7.113 | ppl 138.42 | wps 10003.3 | wpb 1595.8 | bsz 59.6 | num_updates 4184\n",
            "2022-02-22 12:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 4184 updates\n",
            "2022-02-22 12:27:27 | INFO | fairseq.trainer | Saving checkpoint to /content/nmt/model/checkpoints/checkpoint1.pt\n",
            "2022-02-22 12:27:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/nmt/model/checkpoints/checkpoint1.pt\n",
            "2022-02-22 12:27:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint nmt/model/checkpoints/checkpoint1.pt (epoch 1 @ 4184 updates, score 8.247) (writing took 14.350076298999738 seconds)\n",
            "2022-02-22 12:27:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-02-22 12:27:42 | INFO | train | epoch 001 | loss 9.118 | nll_loss 8.203 | ppl 294.61 | wps 3471.5 | ups 1.92 | wpb 1807 | bsz 67.1 | num_updates 4184 | lr 0.000977764 | gnorm 1.327 | train_wall 2104 | gb_free 8.6 | wall 2179\n",
            "2022-02-22 12:27:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4184\n",
            "epoch 002:   0% 0/4184 [00:00<?, ?it/s]2022-02-22 12:27:42 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-02-22 12:27:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 4183/4184 [35:15<00:00,  1.97it/s, loss=7.705, nll_loss=6.6, ppl=97.03, wps=3541.3, ups=1.98, wpb=1791, bsz=69.2, num_updates=8300, lr=0.00069421, gnorm=1.483, train_wall=50, gb_free=8.5, wall=4261]2022-02-22 13:02:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/265 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   0% 1/265 [00:00<02:53,  1.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 2/265 [00:00<01:38,  2.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   1% 3/265 [00:00<01:11,  3.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 4/265 [00:01<00:57,  4.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 5/265 [00:01<00:51,  5.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 6/265 [00:01<00:49,  5.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 7/265 [00:01<00:47,  5.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 9/265 [00:01<00:37,  6.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 10/265 [00:02<00:38,  6.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 11/265 [00:02<00:37,  6.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 12/265 [00:02<00:37,  6.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 13/265 [00:02<00:39,  6.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 14/265 [00:02<00:39,  6.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 15/265 [00:02<00:38,  6.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 16/265 [00:02<00:40,  6.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 17/265 [00:03<00:40,  6.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 18/265 [00:03<00:39,  6.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 19/265 [00:03<00:40,  6.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 20/265 [00:03<00:40,  6.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 21/265 [00:03<00:40,  6.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 22/265 [00:03<00:39,  6.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 23/265 [00:04<00:40,  5.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 24/265 [00:04<00:40,  5.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 25/265 [00:04<00:40,  5.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  10% 26/265 [00:04<00:39,  6.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  10% 27/265 [00:04<00:37,  6.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 28/265 [00:04<00:39,  5.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 29/265 [00:05<00:39,  5.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 30/265 [00:05<00:39,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 31/265 [00:05<00:38,  6.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 32/265 [00:05<00:37,  6.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 33/265 [00:05<00:39,  5.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 34/265 [00:05<00:39,  5.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 35/265 [00:06<00:38,  5.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 36/265 [00:06<00:38,  6.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 37/265 [00:06<00:36,  6.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 38/265 [00:06<00:37,  6.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 39/265 [00:06<00:38,  5.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 40/265 [00:06<00:37,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 41/265 [00:07<00:37,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 42/265 [00:07<00:37,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 43/265 [00:07<00:35,  6.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 44/265 [00:07<00:34,  6.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 45/265 [00:07<00:35,  6.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 46/265 [00:07<00:36,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 47/265 [00:08<00:37,  5.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 48/265 [00:08<00:37,  5.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 49/265 [00:08<00:36,  6.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  19% 50/265 [00:08<00:34,  6.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  19% 51/265 [00:08<00:35,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 52/265 [00:08<00:36,  5.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 53/265 [00:09<00:36,  5.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 54/265 [00:09<00:36,  5.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 55/265 [00:09<00:35,  5.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 56/265 [00:09<00:34,  6.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 57/265 [00:09<00:31,  6.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 58/265 [00:09<00:32,  6.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 59/265 [00:10<00:33,  6.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 60/265 [00:10<00:34,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 61/265 [00:10<00:33,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 62/265 [00:10<00:33,  6.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 63/265 [00:10<00:31,  6.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 64/265 [00:10<00:28,  7.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 65/265 [00:10<00:27,  7.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 66/265 [00:11<00:30,  6.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 67/265 [00:11<00:31,  6.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 68/265 [00:11<00:31,  6.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 69/265 [00:11<00:32,  6.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 70/265 [00:11<00:32,  6.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 71/265 [00:11<00:31,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 72/265 [00:12<00:31,  6.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 73/265 [00:12<00:32,  5.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 74/265 [00:12<00:32,  5.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 75/265 [00:12<00:32,  5.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 76/265 [00:12<00:31,  6.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 77/265 [00:12<00:31,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 78/265 [00:13<00:31,  5.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 79/265 [00:13<00:29,  6.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 80/265 [00:13<00:29,  6.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 81/265 [00:13<00:30,  5.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 82/265 [00:13<00:31,  5.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 83/265 [00:13<00:31,  5.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 84/265 [00:14<00:30,  5.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 85/265 [00:14<00:30,  5.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 86/265 [00:14<00:28,  6.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 87/265 [00:14<00:28,  6.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 88/265 [00:14<00:28,  6.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 89/265 [00:14<00:29,  6.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 90/265 [00:15<00:29,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  34% 91/265 [00:15<00:29,  5.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 92/265 [00:15<00:28,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 93/265 [00:15<00:28,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 94/265 [00:15<00:26,  6.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 95/265 [00:15<00:25,  6.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 96/265 [00:16<00:26,  6.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 97/265 [00:16<00:27,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 98/265 [00:16<00:28,  5.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 99/265 [00:16<00:27,  5.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 100/265 [00:16<00:27,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 101/265 [00:16<00:26,  6.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 102/265 [00:17<00:25,  6.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 103/265 [00:17<00:25,  6.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 104/265 [00:17<00:25,  6.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 105/265 [00:17<00:26,  6.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 106/265 [00:17<00:26,  5.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  40% 107/265 [00:17<00:26,  6.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 108/265 [00:18<00:26,  5.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 109/265 [00:18<00:26,  5.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 110/265 [00:18<00:25,  5.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 111/265 [00:18<00:24,  6.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 112/265 [00:18<00:24,  6.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 113/265 [00:18<00:25,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 114/265 [00:19<00:25,  5.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 115/265 [00:19<00:25,  5.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 116/265 [00:19<00:25,  5.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 117/265 [00:19<00:25,  5.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 118/265 [00:19<00:24,  5.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 119/265 [00:19<00:24,  6.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 120/265 [00:20<00:22,  6.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 121/265 [00:20<00:23,  6.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 122/265 [00:20<00:23,  6.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 123/265 [00:20<00:23,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 124/265 [00:20<00:24,  5.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 125/265 [00:20<00:23,  5.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 126/265 [00:21<00:22,  6.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 127/265 [00:21<00:21,  6.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 128/265 [00:21<00:20,  6.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 129/265 [00:21<00:21,  6.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 130/265 [00:21<00:21,  6.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 131/265 [00:21<00:22,  5.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 132/265 [00:22<00:22,  6.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 133/265 [00:22<00:21,  6.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 134/265 [00:22<00:21,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 135/265 [00:22<00:21,  6.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  51% 136/265 [00:22<00:21,  6.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 137/265 [00:22<00:21,  5.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 138/265 [00:23<00:21,  5.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 139/265 [00:23<00:21,  5.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 140/265 [00:23<00:21,  5.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 141/265 [00:23<00:20,  6.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 142/265 [00:23<00:20,  6.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 143/265 [00:23<00:19,  6.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 144/265 [00:23<00:19,  6.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 145/265 [00:24<00:20,  5.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 146/265 [00:24<00:20,  5.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 147/265 [00:24<00:19,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 148/265 [00:24<00:19,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 149/265 [00:24<00:19,  5.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 150/265 [00:24<00:18,  6.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 151/265 [00:25<00:17,  6.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 152/265 [00:25<00:18,  6.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 153/265 [00:25<00:18,  6.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 154/265 [00:25<00:18,  5.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 155/265 [00:25<00:17,  6.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 156/265 [00:25<00:17,  6.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 157/265 [00:26<00:17,  6.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 158/265 [00:26<00:17,  6.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 159/265 [00:26<00:17,  6.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 160/265 [00:26<00:17,  5.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 161/265 [00:26<00:17,  5.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 162/265 [00:26<00:17,  5.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 163/265 [00:27<00:16,  6.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 164/265 [00:27<00:16,  6.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 165/265 [00:27<00:16,  6.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 166/265 [00:27<00:16,  6.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 167/265 [00:27<00:16,  5.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 168/265 [00:27<00:16,  5.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 169/265 [00:28<00:16,  5.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 170/265 [00:28<00:15,  5.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 171/265 [00:28<00:15,  6.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 172/265 [00:28<00:15,  5.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 173/265 [00:28<00:15,  5.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 174/265 [00:28<00:15,  5.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 175/265 [00:29<00:15,  5.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 176/265 [00:29<00:14,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 177/265 [00:29<00:13,  6.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 178/265 [00:29<00:14,  6.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 179/265 [00:29<00:14,  5.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 180/265 [00:29<00:14,  5.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 181/265 [00:30<00:14,  5.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 182/265 [00:30<00:13,  5.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 183/265 [00:30<00:13,  6.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 184/265 [00:30<00:13,  6.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 185/265 [00:30<00:13,  6.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 186/265 [00:30<00:13,  5.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 187/265 [00:31<00:12,  6.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 188/265 [00:31<00:11,  6.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 189/265 [00:31<00:11,  6.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 190/265 [00:31<00:11,  6.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 191/265 [00:31<00:11,  6.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 192/265 [00:31<00:12,  6.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 193/265 [00:32<00:11,  6.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 194/265 [00:32<00:11,  6.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 195/265 [00:32<00:11,  6.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 196/265 [00:32<00:11,  6.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 197/265 [00:32<00:10,  6.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 198/265 [00:32<00:10,  6.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 199/265 [00:33<00:10,  6.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 200/265 [00:33<00:10,  6.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 201/265 [00:33<00:10,  5.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 202/265 [00:33<00:10,  6.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 203/265 [00:33<00:10,  6.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 204/265 [00:33<00:10,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 205/265 [00:34<00:10,  5.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 206/265 [00:34<00:09,  6.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 207/265 [00:34<00:09,  6.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 208/265 [00:34<00:09,  6.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 209/265 [00:34<00:08,  6.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 210/265 [00:34<00:08,  6.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 211/265 [00:34<00:08,  6.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 212/265 [00:35<00:08,  6.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 213/265 [00:35<00:08,  6.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 214/265 [00:35<00:08,  6.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 215/265 [00:35<00:07,  6.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 216/265 [00:35<00:07,  6.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 217/265 [00:35<00:07,  6.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 218/265 [00:36<00:07,  6.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 219/265 [00:36<00:07,  6.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 220/265 [00:36<00:07,  6.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 221/265 [00:36<00:07,  6.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 222/265 [00:36<00:06,  6.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  84% 223/265 [00:36<00:06,  6.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 224/265 [00:37<00:06,  6.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 225/265 [00:37<00:06,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 226/265 [00:37<00:06,  5.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 227/265 [00:37<00:06,  6.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 228/265 [00:37<00:05,  6.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 229/265 [00:37<00:05,  7.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 230/265 [00:37<00:05,  6.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 231/265 [00:38<00:05,  6.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 232/265 [00:38<00:04,  6.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 233/265 [00:38<00:05,  6.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 234/265 [00:38<00:04,  6.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 235/265 [00:38<00:04,  6.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 236/265 [00:38<00:04,  6.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 237/265 [00:38<00:04,  6.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 238/265 [00:39<00:03,  6.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  90% 239/265 [00:39<00:03,  6.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 240/265 [00:39<00:03,  7.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 241/265 [00:39<00:03,  6.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 242/265 [00:39<00:03,  6.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 243/265 [00:39<00:03,  6.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 244/265 [00:40<00:03,  6.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 245/265 [00:40<00:03,  6.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 246/265 [00:40<00:02,  6.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 247/265 [00:40<00:02,  6.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 248/265 [00:40<00:02,  6.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 249/265 [00:40<00:02,  6.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 250/265 [00:40<00:02,  6.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 251/265 [00:41<00:02,  6.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 252/265 [00:41<00:01,  6.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 253/265 [00:41<00:01,  7.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 254/265 [00:41<00:01,  6.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 255/265 [00:41<00:01,  5.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 256/265 [00:41<00:01,  5.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 257/265 [00:42<00:01,  5.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 258/265 [00:42<00:01,  6.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 259/265 [00:42<00:01,  5.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 260/265 [00:42<00:00,  6.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 261/265 [00:42<00:00,  6.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 262/265 [00:42<00:00,  6.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 263/265 [00:42<00:00,  6.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 264/265 [00:43<00:00,  7.09it/s]\u001b[A\n",
            "                                                                          \u001b[A2022-02-22 13:03:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.668 | nll_loss 6.435 | ppl 86.53 | wps 9900.4 | wpb 1595.8 | bsz 59.6 | num_updates 8368 | best_loss 7.668\n",
            "2022-02-22 13:03:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 8368 updates\n",
            "2022-02-22 13:03:41 | INFO | fairseq.trainer | Saving checkpoint to /content/nmt/model/checkpoints/checkpoint2.pt\n",
            "2022-02-22 13:03:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/nmt/model/checkpoints/checkpoint2.pt\n",
            "2022-02-22 13:03:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint nmt/model/checkpoints/checkpoint2.pt (epoch 2 @ 8368 updates, score 7.668) (writing took 14.564593664999848 seconds)\n",
            "2022-02-22 13:03:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-02-22 13:03:56 | INFO | train | epoch 002 | loss 7.898 | nll_loss 6.817 | ppl 112.74 | wps 3477.2 | ups 1.92 | wpb 1807 | bsz 67.1 | num_updates 8368 | lr 0.000691384 | gnorm 1.28 | train_wall 2099 | gb_free 8.7 | wall 4353\n",
            "2022-02-22 13:03:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 4184\n",
            "epoch 003:   0% 0/4184 [00:00<?, ?it/s]2022-02-22 13:03:56 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-02-22 13:03:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 530, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 305, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 929, in train_step\n",
            "    grad_norm = self.clip_grad_norm(self.cfg.optimization.clip_norm)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 1264, in clip_grad_norm\n",
            "    clip_norm, aggregate_norm_fn=agg_norm_fn if should_agg_norm else None\n",
            "  File \"/content/fairseq/fairseq/optim/fairseq_optimizer.py\", line 112, in clip_grad_norm\n",
            "    return utils.clip_grad_norm_(self.params, max_norm, aggregate_norm_fn)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/fairseq/fairseq/utils.py\", line 385, in clip_grad_norm_\n",
            "    [torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]\n",
            "  File \"/content/fairseq/fairseq/utils.py\", line 385, in <listcomp>\n",
            "    [torch.norm(g, p=2, dtype=torch.float32).to(device) for g in grads]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/functional.py\", line 1467, in norm\n",
            "    return _VF.norm(input, p, _dim, keepdim=keepdim, dtype=dtype)  # type: ignore[attr-defined]\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#解码\n",
        "fairseq支持两种解码命令 generate和interactive \n",
        "\n",
        "*     generate使用二进制文件，符合自己做实验如WMT数据集上\n",
        "*     interactive  用于文本文件，符合比赛常见\n"
      ],
      "metadata": {
        "id": "5aPDJktwaiP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#用于保存所以结果文件\n",
        "!mkdir nmt/result"
      ],
      "metadata": {
        "id": "wTwo9_uFbzlr"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#生成式解码\n",
        "!fairseq-generate nmt/data/data-bin\\\n",
        "  --path nmt/model/checkpoints/checkpoint_best.pt \\\n",
        "  --batch-size 128 --beam 8 > nmt/result/bestbeam8.txt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtkmiqkJbIaA",
        "outputId": "e5fb5af4-1abe-424e-b301-816b9c4bd2f8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-22 13:07:05 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-02-22 13:07:07 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'nmt/model/checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'nmt/data/data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-22 13:07:08 | INFO | fairseq.tasks.translation | [zh] dictionary: 37080 types\n",
            "2022-02-22 13:07:08 | INFO | fairseq.tasks.translation | [en] dictionary: 30864 types\n",
            "2022-02-22 13:07:08 | INFO | fairseq_cli.generate | loading model(s) from nmt/model/checkpoints/checkpoint_best.pt\n",
            "2022-02-22 13:07:10 | INFO | fairseq.data.data_utils | loaded 15,630 examples from: nmt/data/data-bin/test.zh-en.zh\n",
            "2022-02-22 13:07:10 | INFO | fairseq.data.data_utils | loaded 15,630 examples from: nmt/data/data-bin/test.zh-en.en\n",
            "2022-02-22 13:07:10 | INFO | fairseq.tasks.translation | nmt/data/data-bin test zh-en 15630 examples\n",
            "  0% 0/124 [00:00<?, ?it/s]/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "2022-02-22 13:16:29 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-22 13:16:29 | INFO | fairseq_cli.generate | Translated 15,630 sentences (297,086 tokens) in 436.3s (35.82 sentences/s, 680.86 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#交互式解码\n",
        "!fairseq-interactive nmt/data/data-bin --input nmt/data/test.zh \\\n",
        "      --path nmt/model/checkpoints/checkpoint_best.pt \\\n",
        "      --batch-size 1 --beam 8 --remove-bpe >nmt/result/bestbeam8_act.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzhgJ9FycDis",
        "outputId": "0b541f0f-6c99-40e2-b01c-a54b12a04637"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-22 13:17:57 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-02-22 13:18:00 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'nmt/model/checkpoints/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 1, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'nmt/data/test.zh'}, 'model': None, 'task': {'_name': 'translation', 'data': 'nmt/data/data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-02-22 13:18:00 | INFO | fairseq.tasks.translation | [zh] dictionary: 37080 types\n",
            "2022-02-22 13:18:00 | INFO | fairseq.tasks.translation | [en] dictionary: 30864 types\n",
            "2022-02-22 13:18:00 | INFO | fairseq_cli.interactive | loading model(s) from nmt/model/checkpoints/checkpoint_best.pt\n",
            "2022-02-22 13:18:06 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-02-22 13:18:06 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "/content/fairseq/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/content/fairseq/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/fairseq-interactive\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-interactive')())\n",
            "  File \"/content/fairseq/fairseq_cli/interactive.py\", line 313, in cli_main\n",
            "    distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/interactive.py\", line 228, in main\n",
            "    generator, models, sample, constraints=constraints\n",
            "  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 538, in inference_step\n",
            "    models, sample, prefix_tokens=prefix_tokens, constraints=constraints\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/fairseq/fairseq/sequence_generator.py\", line 189, in generate\n",
            "    return self._generate(sample, **kwargs)\n",
            "  File \"/content/fairseq/fairseq/sequence_generator.py\", line 332, in _generate\n",
            "    self.model.reorder_incremental_state(incremental_states, reorder_state)\n",
            "  File \"/content/fairseq/fairseq/sequence_generator.py\", line 878, in reorder_incremental_state\n",
            "    incremental_states[i], new_order\n",
            "  File \"/content/fairseq/fairseq/models/fairseq_incremental_decoder.py\", line 99, in reorder_incremental_state_scripting\n",
            "    result = module.reorder_incremental_state(incremental_state, new_order)\n",
            "  File \"/content/fairseq/fairseq/modules/multihead_attention.py\", line 585, in reorder_incremental_state\n",
            "    input_buffer[k] = input_buffer_k.index_select(0, new_order)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#后处理\n",
        "*      将解码生成文件中译文与正确答案抽取出来\n",
        "*      去除bpe符合\n",
        "*      恢复大小写正常"
      ],
      "metadata": {
        "id": "tIA0qkDNd9pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#从解码文件中抽取\n",
        "!grep ^H nmt/result/bestbeam8.txt |cut -f3 >nmt/result/predict.bpe.en\n",
        "!grep ^T nmt/result/bestbeam8.txt |cut -f2 >nmt/result/answer.bpe.en"
      ],
      "metadata": {
        "id": "y2qXWJJueJZU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 30 nmt/result/bestbeam8.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo3tcXU1lxDh",
        "outputId": "d8b40b92-6953-4db0-fe00-06510f872d98"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S-716\t北约 的 新 边界\n",
            "T-716\tNATO &apos;s New Frontier\n",
            "H-716\t-1.6583664417266846\tnew YORK - Europe is the world &apos;s largest country .\n",
            "D-716\t-1.6583664417266846\tnew YORK - Europe is the world &apos;s largest country .\n",
            "P-716\t-2.7592 -0.5841 -0.0321 -3.3731 -2.1757 -2.4677 -2.9088 -0.8209 -1.2908 -2.5991 -0.8637 -0.0251\n",
            "S-13590\t危机 会 发生 。\n",
            "T-13590\tcrises occur .\n",
            "H-13590\t-1.9613300561904907\tthe question now is whether the crisis will be .\n",
            "D-13590\t-1.9613300561904907\tthe question now is whether the crisis will be .\n",
            "P-13590\t-1.8925 -3.6307 -2.3272 -1.0133 -0.7788 -0.9914 -2.4545 -2.4382 -2.0384 -3.9881 -0.0216\n",
            "S-11532\t可以 省钱 的 补贴\n",
            "T-11532\tsubsidies that Save\n",
            "H-11532\t-1.9867991209030151\tto be sure , the government will need to pay a high price .\n",
            "D-11532\t-1.9867991209030151\tto be sure , the government will need to pay a high price .\n",
            "P-11532\t-5.3902 -2.0141 -0.4095 -0.0624 -2.2487 -2.9832 -2.7457 -1.8918 -0.2061 -2.6012 -3.9066 -3.0243 -1.1542 -1.1426 -0.0213\n",
            "S-8019\t天安门 事件 的 教训\n",
            "T-8019\tlessons from Tiananmen\n",
            "H-8019\t-1.7837539911270142\tthis is the case of the history of the twentieth century .\n",
            "D-8019\t-1.7837539911270142\tthis is the case of the history of the twentieth century .\n",
            "P-8019\t-3.7972 -1.4006 -2.4912 -2.9220 -1.1447 -1.5107 -4.4261 -0.1541 -1.3260 -3.5114 -0.0651 -0.4184 -0.0214\n",
            "S-7122\t没有 约束 的 以色列\n",
            "T-7122\tIsrael Un@@ bound\n",
            "H-7122\t-1.7128808498382568\tIsrael is no longer a Palestinian state .\n",
            "D-7122\t-1.7128808498382568\tIsrael is no longer a Palestinian state .\n",
            "P-7122\t-1.6540 -2.6795 -3.7651 -0.3640 -2.9955 -2.6860 -0.5063 -0.7452 -0.0203\n",
            "S-5740\t全球化 之下 的 政府\n",
            "T-5740\tglobalization &apos;s Government\n",
            "H-5740\t-2.308713436126709\tthe World Economic Forum of Economic Forum\n",
            "D-5740\t-2.308713436126709\tthe World Economic Forum of Economic Forum\n",
            "P-5740\t-1.6863 -4.9450 -1.2055 -0.8111 -2.9609 -2.5923 -2.8315 -1.4371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#去除bpe符合\n",
        "!sed -r 's/(@@ )| (@@ ?$)//g' < nmt/result/predict.bpe.en  > nmt/result/predict.tok.true.en\n",
        "!sed -r 's/(@@ )| (@@ ?$)//g' < nmt/result/answer.bpe.en  > nmt/result/answer.tok.true.en\n"
      ],
      "metadata": {
        "id": "c4wLSJlde3yi"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#恢复大小写正常\n",
        "\n",
        "!mosesdecoder/scripts/recaser/detruecase.perl <nmt/result/predict.tok.true.en >nmt/result/predict.tok.en\n",
        "!mosesdecoder/scripts/recaser/detruecase.perl <nmt/result/answer.tok.true.en >nmt/result/answer.tok.en"
      ],
      "metadata": {
        "id": "kxinmxPUduMz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#评估\n",
        "\n",
        "翻译评估主要使用bleu值\n",
        "\n",
        "*     multi-bleu\n",
        "*     sacrebleu"
      ],
      "metadata": {
        "id": "qB75ScOudS9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mutli-bleu 在detokenize前进行评价\n",
        "\n",
        "!mosesdecoder/scripts/generic/multi-bleu.perl -lc nmt/result/answer.tok.en <nmt/result/predict.tok.en\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHtx-1PVdfwb",
        "outputId": "bc395ab9-5635-4f40-fe45-f5292fdd296a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU = 1.69, 29.7/4.1/1.0/0.3 (BP=0.666, ratio=0.711, hyp_len=281454, ref_len=395734)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sacrebleu 在detokenize后进行评价,需要安装对应库\n",
        "\n",
        "#!sacrebleu ref.detok.txt -i output.detok.txt -m bleu -b w 4\n",
        "\n",
        "#cat output.detok.txt | sacrebleu ref.detok.txt -m bleu -b -w 4"
      ],
      "metadata": {
        "id": "Fau3vvNggT5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#使用detokenizer 预测得到纯文本\n",
        "\n",
        "!mosesdecoder/scripts/tokenizer/detokenizer.perl -l en < nmt/result/predict.tok.en >nmt/result/predict.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5c3CoimgyZr",
        "outputId": "7acb533b-f79b-4578-f4bc-928bf372221b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 30 nmt/result/predict.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKj8LT_ynJKw",
        "outputId": "2a37f7d9-e5e0-406d-f40b-949e96ea2e22"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New YORK - Europe is the world's largest country.\n",
            "The question now is whether the crisis will be.\n",
            "To be sure, the government will need to pay a high price.\n",
            "This is the case of the history of the twentieth century.\n",
            "Israel is no longer a Palestinian state.\n",
            "The World Economic Forum of Economic Forum\n",
            "The same is true of the \"soft power.\"\n",
            "Egypt is the first country in the country.\n",
            "A few weeks later, the police were killed.\n",
            "The Greek economy is the first option.\n",
            "The first of the twentieth century of the twentieth century was the first time of the twentieth century.\n",
            "The World Health Organization, for example, is the most part of the world.\n",
            "The World Bank is the world's largest economy.\n",
            "In the Arab world, the Arab Spring of the Arab Spring has become a major threat to democracy.\n",
            "This is the case of the South China Sea.\n",
            "The first option is to reduce carbon emissions.\n",
            "The US is the world's largest ally.\n",
            "Europe is the world.\n",
            "That is not the case.\n",
            "New YORK - In the last two years, China has been the world's largest economy.\n",
            "So what will be done?\n",
            "Washington, DC - US President Barack Obama.\n",
            "The World Bank of China, for example, is the world's largest economy.\n",
            "The third is the World Bank.\n",
            "Is it?\n",
            "The question is whether it can be said to be done.\n",
            "New YORK - In the last decade, the economy is expected to grow.\n",
            "One of the most important is unemployment.\n",
            "The United Kingdom's Brexit referendum.\n",
            "The United States is the world's largest country.\n"
          ]
        }
      ]
    }
  ]
}